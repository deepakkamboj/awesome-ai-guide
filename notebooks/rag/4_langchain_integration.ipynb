{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structures in python\n",
    "* String\n",
    "* List\n",
    "* Tuples\n",
    "* Dictionaries\n",
    "* Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*args\n",
    "\n",
    "**kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *args -> positional arguments\n",
    "\n",
    "def get_prod(*args):\n",
    "    res = 1\n",
    "    for arg in args:\n",
    "        res = res*arg\n",
    "    return res\n",
    "\n",
    "get_prod(1, 4, 5, 2, 3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **kwargs\n",
    "\n",
    "def greet(**kwargs):\n",
    "    greeting = \"Hello\"\n",
    "    if \"name\" in kwargs:\n",
    "        greeting += f\", {kwargs['name']}\"\n",
    "    if 'age' in  kwargs:\n",
    "        greeting += f\", You are {kwargs['age']} years old\"\n",
    "    if \"location\" in kwargs:\n",
    "        greeting += f\" from {kwargs['location']}\"\n",
    "    greeting+=\"!\"\n",
    "    return greeting\n",
    "\n",
    "print(greet(name='John'))\n",
    "print(greet(name='John', age=24))\n",
    "print(greet(name=\"John\", location=\"New york\"))\n",
    "print(greet(name=\"John\", age=24, location=\"New york\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args(*args):\n",
    "    return args\n",
    "\n",
    "def check_kwargs(**kwargs):\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_args(1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_kwargs(name=\"John\", age=24, location=\"New york\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorators\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "get_curr_time = lambda : datetime.now().time().strftime(\"%H:%M:%S\")\n",
    "\n",
    "def timer(func):\n",
    "    def get_timings(*args, **kwargs):\n",
    "        start_time = get_curr_time()\n",
    "        val = func(*args, **kwargs)\n",
    "        time.sleep(2)\n",
    "        end_time = get_curr_time()\n",
    "        print(f\"Start time: {start_time} | End time: {end_time} \")\n",
    "        return val\n",
    "    return get_timings\n",
    "\n",
    "@timer\n",
    "def get_sq(a):\n",
    "    return a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sq(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"Write a film story outline on the topic: {topic}\"\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(input={\"topic\": \"Growth of India\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt.format(**{\"topic\": \"Growth of India\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = chat_prompt | chat\n",
    "\n",
    "lcel_chain.invoke(input={\"topic\": \"Growth of India\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_line_template = \"Write a film story outline on the topic: {topic}\"\n",
    "story_line_prompt = ChatPromptTemplate.from_template(template=story_line_template)\n",
    "story_line_chain = LLMChain(llm=chat, prompt=story_line_prompt)\n",
    "\n",
    "full_story_template = \"Write a short film story on the given story line: {story_line}\"\n",
    "full_story_prompt = ChatPromptTemplate.from_template(template=full_story_template)\n",
    "full_story_chain = LLMChain(llm=chat, prompt=full_story_prompt)\n",
    "\n",
    "reviewer_template = \"Act as a reviewer of rotten tomatoes and rate the given story: {full_story}\"\n",
    "reviewer_prompt = ChatPromptTemplate.from_template(template=reviewer_template)\n",
    "reviewer_chain = LLMChain(llm=chat, prompt=reviewer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "film_chain = SimpleSequentialChain(\n",
    "    chains=[story_line_chain, full_story_chain, reviewer_chain],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = film_chain.run(\"Growth of India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_film_chain = story_line_chain | full_story_chain | reviewer_chain\n",
    "\n",
    "lcel_film_chain.invoke(input={\"topic\": \"Growth of India\", \"story_line\": \"Growth of India\", \"full_story\": \"Growth of India\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def get_story_line(response):\n",
    "    return {\"story_line\": response['text']}\n",
    "\n",
    "def get_full_story(response):\n",
    "    return {\"full_story\": response['text']}\n",
    "\n",
    "story_line_lcel_chain = story_line_chain | RunnableLambda(get_story_line)\n",
    "full_story_lcel_chain = full_story_chain | RunnableLambda(get_full_story)\n",
    "\n",
    "lcel_film_chain = story_line_lcel_chain | full_story_lcel_chain | reviewer_chain\n",
    "\n",
    "lcel_film_chain.invoke(input={\"topic\": \"Growth of India\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_line_template = \"Write a film story outline on the topic: {topic}\"\n",
    "story_line_prompt = ChatPromptTemplate.from_template(template=story_line_template)\n",
    "story_line_chain = LLMChain(llm=chat, prompt=story_line_prompt, output_key=\"story_line\")\n",
    "\n",
    "full_story_template = \"Write a short film story on the given story line: {story_line}\"\n",
    "full_story_prompt = ChatPromptTemplate.from_template(template=full_story_template)\n",
    "full_story_chain = LLMChain(llm=chat, prompt=full_story_prompt, output_key=\"full_story\")\n",
    "\n",
    "reviewer_template = \"Act as a reviewer of rotten tomatoes and rate the given story: {full_story}\\n and the topic is: {topic}\"\n",
    "reviewer_prompt = ChatPromptTemplate.from_template(template=reviewer_template)\n",
    "reviewer_chain = LLMChain(llm=chat, prompt=reviewer_prompt, output_key=\"reviwer_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = story_line_chain | full_story_chain | reviewer_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain.invoke(input={\"topic\": \"Growth of India\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = SequentialChain(\n",
    "    chains=[story_line_chain, full_story_chain, reviewer_chain],\n",
    "    input_variables=['topic'],\n",
    "    output_variables = ['story_line', 'full_story', 'reviwer_response'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "seq_chain.invoke(input={\"topic\": \"Growth of India\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def square(a):\n",
    "    return a**2\n",
    "\n",
    "sq_runnable = RunnableLambda(square)\n",
    "\n",
    "sq_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sq_runnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_runnable.invoke(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "runnable_sq_pass = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_sq_pass.invoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def times2(x):\n",
    "    return x*2\n",
    "\n",
    "\n",
    "def times3(y):\n",
    "    return y*3\n",
    "\n",
    "\n",
    "# def add(res_dict):\n",
    "#     return res_dict['a'] + res_dict['b']\n",
    "def add(res_dict):\n",
    "    res = 0\n",
    "    for k, v in res_dict.items():\n",
    "        res+=v\n",
    "    return res\n",
    "\n",
    "runnable_times2 = RunnableLambda(times2)\n",
    "runnable_times3 = RunnableLambda(times3)\n",
    "runnable_sum = RunnableLambda(add)\n",
    "\n",
    "# par_chain = RunnableParallel({\"a\": itemgetter('x') | runnable_times2, \n",
    "#                               \"b\": itemgetter('y') | runnable_times3})\n",
    "\n",
    "par_chain = RunnableParallel(\n",
    "    {\"a\": itemgetter('x') | runnable_times2, \n",
    "     \"b\": itemgetter('y') | runnable_times3,\n",
    "     \"x\": itemgetter('x') | RunnablePassthrough(),\n",
    "     \"y\": itemgetter('y') | RunnablePassthrough(),\n",
    "     })\n",
    "\n",
    "calc_chain = par_chain | runnable_sum\n",
    "\n",
    "calc_chain.invoke(input={\"x\": 2, \"y\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnableLambdaTest:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        \n",
    "    def __or__(self, other_runnable_obj):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other_runnable_obj.invoke(self.func(*args, **kwargs))\n",
    "        return RunnableLambdaTest(chained_func)\n",
    "    \n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "    \n",
    "def times2(a):\n",
    "    return a*2\n",
    "\n",
    "def times3(b): \n",
    "    return b*3\n",
    "\n",
    "runnable_2 = RunnableLambdaTest(times2)\n",
    "runnable_3 = RunnableLambdaTest(times3)\n",
    "\n",
    "# test_chain = runnable_2.__or__(runnable_3)\n",
    "\n",
    "test_chain = runnable_2 | runnable_3\n",
    "\n",
    "test_chain.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMRouterChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task types\n",
    "\n",
    "psy_name = \"Psychologist\" # Who\n",
    "\n",
    "psy_description = \"Helps with any psychological issues\" # What\n",
    "\n",
    "psy_template = \"\"\"You are a helpful psychologist who wants to help the user to make them feel better.\n",
    "You always give 3 simple and most appropriate remedies to the user based on their issue.\n",
    "The user issue is mentioned in the below triple backticks:\n",
    "```\n",
    "{input}\n",
    "```\n",
    "\"\"\" # How\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "phy_name = \"General Physician\"\n",
    "\n",
    "phy_description = \"Helps with any General Physical Health issues\"\n",
    "\n",
    "phy_template = \"\"\"You are a helpful general physician who always suggests home remedies for any issues.\n",
    "You always suggest 5 most appropriate remedies to the user based on their issue.\n",
    "The user issue is mentioned in the below triple backticks:\n",
    "```\n",
    "{input}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_dict = {psy_name: chat, phy_name: chat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_info_list = [\n",
    "    {\n",
    "        \"name\": psy_name,\n",
    "        \"description\": psy_description,\n",
    "        \"template\": psy_template\n",
    "    }, \n",
    "    {\n",
    "        \"name\": phy_name,\n",
    "        \"description\": phy_description,\n",
    "        \"template\": phy_template\n",
    "    }, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "destination_descriptions = \"\"\n",
    "\n",
    "for prompt_info in prompt_info_list:\n",
    "    name = prompt_info['name']\n",
    "    template = prompt_info['template']\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "    llm = llm_dict[name]\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "    \n",
    "    destination_descriptions += f\"{prompt_info['name']}: {prompt_info['description']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(destination_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router import MultiPromptChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destination_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = PromptTemplate(template=router_template, input_variables=[], output_parser=RouterOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = LLMRouterChain.from_llm(llm=chat, prompt=router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "default_chain = ConversationChain(llm=chat, output_key='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.run(\"I could not sleep last night\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.run(\"I couldn't walk properly since I hit my leg to the bed yesterday\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.run(\"Taj mahal is beautiful\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.run(\"I don't like taj mahal\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MathChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMMathChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_chain.run(\"What is 23 multiplied by 4 and subtracted by 18?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PALChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.pal_chain import PALChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_chain = PALChain.from_math_prompt(llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_chain.run(\"A train travels at a speed of 60km/h and covers a certain distance in 3 hours. What is the distance covered by the train?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "from langchain.chains import MultiRetrievalQAChain\n",
    "\n",
    "chain = MultiRetrievalQAChain.from_retrievers(\n",
    "    llm=chat,\n",
    "    retriever_infos, \n",
    "    default_chain\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
