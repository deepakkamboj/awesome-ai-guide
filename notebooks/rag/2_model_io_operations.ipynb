{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key loading\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open('../openai_api_key.txt', 'r') as f:\n",
    "    api_key = f.read()\n",
    "    \n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text completion model\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prompt\n",
    "\n",
    "prompt = \"The impact of the globalization on diverse cultures can be explained as:\"\n",
    "\n",
    "response = llm(prompt=prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple prompts\n",
    "\n",
    "prompts = [\n",
    "    \"The impact of the globalization on diverse cultures can be explained as:\",\n",
    "    \"Ecosystems maintains biodiversity as follows:\"\n",
    "]\n",
    "\n",
    "response = llm.generate(prompts=prompts)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_list in response.generations:\n",
    "    gen = gen_list[0]\n",
    "    text = gen.text\n",
    "    print(text)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Usage Info\n",
    "\n",
    "response.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Caching\n",
    "\n",
    "from langchain.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In memory caching\n",
    "\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite caching\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path='../models/cache.db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(\"Give all the details about Bali...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(\"Give all the details about Bali...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat model: Schema\n",
    "\n",
    "# SystemMessage: Role assigned to the AI.\n",
    "# HumanMessage: We are asking\n",
    "# AIMessage: AI responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages=[HumanMessage(content='What is the longest river in the world?')])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding system message\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='Act as a funny anthropologist'),\n",
    "    HumanMessage(content=\"The impact of the globalization on diverse cultures can be explained as:\")\n",
    "]\n",
    "\n",
    "response = chat(messages=messages)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "response = chat(\n",
    "    messages=[\n",
    "        SystemMessage(content='You are an angry doctor'),\n",
    "        HumanMessage(content='Explain the digestion process in human bodies')\n",
    "    ],\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.2,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few shot prompting\n",
    "\n",
    "system_message = \"You are a funny doctor\"\n",
    "\n",
    "patient_dialogue1 = \"Doctor, I have been feeling a bit under the weather lately.\"\n",
    "sample_response1 = \"Under the weather? Did you try checking the forecast before stepping out? You might need a weather app prescription!\"\n",
    "\n",
    "patient_dialogue2 = \"My throat has been sore, and I have a cough.\"\n",
    "sample_response2 = \"The classic sore throat symphony! I recommend a strong dose of chicken soup and a dialy karaoke session. Sing it out, and your throat will thank you.\"\n",
    "\n",
    "patient_dialogue3 = \"I have a headache.\"\n",
    "sample_response3 = \"Headache, you say? Have you tried negotiating with it? Maybe it's just looking for a better job inside your brain!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    \n",
    "    HumanMessage(content=patient_dialogue1),\n",
    "    AIMessage(sample_response1),\n",
    "    \n",
    "    HumanMessage(content=patient_dialogue2),\n",
    "    AIMessage(sample_response2),\n",
    "    \n",
    "    HumanMessage(content=patient_dialogue3),\n",
    "    AIMessage(sample_response3),\n",
    "    \n",
    "    HumanMessage(content='I have a stomach pain')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages=messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_funny_doctor_response(prompt):\n",
    "    system_message = \"You are a funny doctor\"\n",
    "\n",
    "    patient_dialogue1 = \"Doctor, I have been feeling a bit under the weather lately.\"\n",
    "    sample_response1 = \"Under the weather? Did you try checking the forecast before stepping out? You might need a weather app prescription!\"\n",
    "\n",
    "    patient_dialogue2 = \"My throat has been sore, and I have a cough.\"\n",
    "    sample_response2 = \"The classic sore throat symphony! I recommend a strong dose of chicken soup and a dialy karaoke session. Sing it out, and your throat will thank you.\"\n",
    "\n",
    "    patient_dialogue3 = \"I have a headache.\"\n",
    "    sample_response3 = \"Headache, you say? Have you tried negotiating with it? Maybe it's just looking for a better job inside your brain!\"\n",
    "    \n",
    "    messages = [\n",
    "        # SystemMessage(content=system_message),\n",
    "        \n",
    "        HumanMessage(content=patient_dialogue1),\n",
    "        AIMessage(sample_response1),\n",
    "        \n",
    "        HumanMessage(content=patient_dialogue2),\n",
    "        AIMessage(sample_response2),\n",
    "        \n",
    "        HumanMessage(content=patient_dialogue3),\n",
    "        AIMessage(sample_response3),\n",
    "        \n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    response = chat(messages=messages)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "get_funny_doctor_response('I have a stomach pain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "\n",
    "# Cross questioning bot\n",
    "\n",
    "# Lame humor bot\n",
    "\n",
    "# TASKS\n",
    "\n",
    "# Create a GitHub account\n",
    "# Write a blog on few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open('../openai_api_key.txt', 'r') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = f.read()\n",
    "    \n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format strings\n",
    "\n",
    "prompt_template = \"write an essay on {topic}\"\n",
    "\n",
    "prompt = prompt_template.format(topic='data science')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt_template.format(topic='science'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nScience is a systematic and logical approach to understanding the natural world. It is a method of acquiring knowledge through observation, experimentation, and analysis. Science is a broad field that encompasses many disciplines such as biology, chemistry, physics, and environmental science. It has played a crucial role in shaping the modern world and has revolutionized our understanding of the universe.\\n\\nOne of the key components of science is the scientific method, a systematic approach to solving problems and answering questions about the natural world. This method involves making observations, forming a hypothesis, conducting experiments, and analyzing the results. Through this process, scientists are able to test their ideas and theories and come to evidence-based conclusions.\\n\\nScience has made significant contributions to human progress and has transformed our lives in unimaginable ways. From the invention of the wheel to the development of modern medicine, science has been the driving force behind many technological advancements. It has enabled us to understand and manipulate the world around us, leading to the development of new technologies, medicines, and materials.\\n\\nOne of the most significant contributions of science is in the field of medicine. Through scientific research, diseases that were once deadly have been eradicated, and life expectancy has increased significantly. The discovery of vaccines, antibiotics, and other medical treatments has saved countless lives and improved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-string literals\n",
    "\n",
    "topic = 'data science'\n",
    "\n",
    "prompt = f\"Write an essay on {topic}\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(topic):\n",
    "    \n",
    "    prompt = f\"Write an essay on {topic}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "get_prompt(topic='data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = 'Write an essay on {topic}'\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(topic='data science')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template = 'Write an essay on {topic} in {num_words} words'\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(topic='data science', num_words=200)\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.save('../output/prompt_template.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt('../output/prompt_template.json')\n",
    "\n",
    "loaded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loaded_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "human_template = \"Write an essay on {topic}\"\n",
    "\n",
    "human_prompt_template = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "human_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt_template])\n",
    "\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.format_prompt(topic='data science')\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(messages = prompt.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate, # Few shot prompting with templates\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Message Prompt Template\n",
    "\n",
    "system_template = \"You are a {character}\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "system_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"Tell me the impact of {food_item} on human body when consumed regularly\"\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "human_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_prompt.format_prompt(character='yoga guru', food_item='ice cream')\n",
    "\n",
    "print(chat(messages=prompt.to_messages()).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "\n",
    "# Role - shoper\n",
    "# Sales script for items\n",
    "\n",
    "system_template = \"You are a {character}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"Write a selling script to sell a {product1} and a {product2} together.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "prompt = chat_prompt.format_prompt(character='Cloth seller', product1='saree', product2='tie')\n",
    "\n",
    "response = chat(messages=prompt.to_messages())\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open('../openai_api_key.txt') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = f.read()\n",
    "    \n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format instructions\n",
    "# Parse (eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "human_template = \"{human_message}\\n{format_instructions}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "prompt = chat_prompt.format_prompt(human_message=\"What are the 7 wonders?\", format_instructions=output_parser.get_format_instructions())\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.messages\n",
    "\n",
    "response = chat(messages=messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_parser.parse(response.content)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if the parser fails?\n",
    "\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"{human_message}\\n{format_instructions}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "prompt = chat_prompt.format_prompt(human_message=\"When was Jesus Christ born?\", format_instructions=output_parser.get_format_instructions())\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.messages\n",
    "\n",
    "response = chat(messages=messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_parser.parse(response.content)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutputFixingParser\n",
    "\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=output_parser, llm=chat)\n",
    "\n",
    "fixed_output = fixing_parser.parse(response.content)\n",
    "\n",
    "fixed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixing_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fixed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chance in range(1, 10):\n",
    "    try:\n",
    "        fixed_output = fixing_parser.parse(response.content)\n",
    "    except:\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "\n",
    "fixed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "rseponse_schema = [\n",
    "    ResponseSchema(\n",
    "        name='answer', description='answer to the user\"s question'\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name='source',\n",
    "        description='source used to answer the user\"s question, should be a website.'\n",
    "    )\n",
    "]\n",
    "\n",
    "rseponse_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our output_parser\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(rseponse_schema)\n",
    "\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"{human_message}\\n{format_instructions}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "prompt = chat_prompt.format_prompt(human_message=\"What's the world's largest man made structure?\", format_instructions=output_parser.get_format_instructions())\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.to_messages()\n",
    "\n",
    "response = chat(messages=messages)\n",
    "\n",
    "output = output_parser.parse(response.content)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        \n",
    "john = Student(name=1.0)\n",
    "john.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Student(BaseModel):\n",
    "    name: str\n",
    "    \n",
    "john = Student(name=1)\n",
    "john.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Car(BaseModel):\n",
    "    name: str = Field(description='Name of the car')\n",
    "    model_number: str = Field(description='Model number of the car')\n",
    "    features: List[str] = Field(description='List of features of the car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Car)\n",
    "\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"{human_message}\\n{format_instructions}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "prompt = chat_prompt.format_prompt(human_message=\"Tell me about the most expensive car in the world\",\n",
    "                                   format_instructions=output_parser.get_format_instructions())\n",
    "\n",
    "messages = prompt.to_messages()\n",
    "\n",
    "response = chat(messages=messages)\n",
    "\n",
    "output = output_parser.parse(response.content)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.model_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest update\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../openai_api_key.txt') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car(BaseModel):\n",
    "    name: str = Field(description=\"Name of the car\")\n",
    "    model_number: str = Field(description=\"Model number of the car\")\n",
    "    features: List[str] = Field(description=\"List of features of the car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "model_with_structure = model.with_structured_output(Car)\n",
    "model_with_structure.invoke('Tell me about the most expensive car in the world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ai21 import AI21ContextualAnswers\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "with open('../openai_api_key.txt', 'r') as f:\n",
    "    os.environ['OPENAI_API_KEY'] = f.read()\n",
    "\n",
    "tsm = AI21ContextualAnswers()\n",
    "chain = tsm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Erick likes to ride bikes\",\n",
    "    \"question\": \"Who likes bikes\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartChef(BaseModel):\n",
    "    name: str = Field(description='Name of the dish')\n",
    "    ingredients: dict = Field(description='Python dictionary of ingredients and their corresponding quantities as key and values of the python dictionary respectively.')\n",
    "    instructions: List[str] = Field(description='Python list of instructions to prepare the dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=SmartChef)\n",
    "\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response\n",
    "\n",
    "human_template = \"\"\"I have the following list of food items:\n",
    "\n",
    "{food_items}\n",
    "\n",
    "Suggest me a recipe only using these food items\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "prompt = chat_prompt.format_prompt(\n",
    "    food_items=\"rice, onion, tomatoes, ginger garlic paste, turmeric, cumin seeds\",\n",
    "    format_instructions=output_parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "messages = prompt.to_messages()\n",
    "\n",
    "response = chat(messages=messages)\n",
    "\n",
    "output = output_parser.parse(response.content)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks\n",
    "\n",
    "# Create a Hugging face account\n",
    "# create a heroku account\n",
    "\n",
    "# Build a project\n",
    "\n",
    "# Real time text translation (import audio in gradio)\n",
    "# Text Summarization tool (topic wise summarizer)\n",
    "# Q&A Sysytem\n",
    "# Travel Planner\n",
    "# Tweet Responder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
