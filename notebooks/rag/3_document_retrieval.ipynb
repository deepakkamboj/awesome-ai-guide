{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt -> LLM -> Response\n",
        "# (New data/external data) + Prompt -> LLM -> Response\n",
        "# How are we retrieving this information to add it to the prompt\n",
        "# Copy paste new information\n",
        "# Can programmatically read the new information\n",
        "# Can scrape the information\n",
        "\n",
        "### CONTEXT WINDOW ###\n",
        "# chunk -> -> embeddings -> DB -> Retrieve appropriate information to add it in the prompt\n",
        "# Appropriate -> data -> embeddings (models) -> DB -> perform query (similarity) -> get relevant content data\n",
        "# appropriate + prompt -> LLM -> Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSV Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import CSVLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.path.isdir(\"../datasets/sns_datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = CSVLoader(file_path=\"../datasets/sns_datasets/titanic.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = CSVLoader(file_path=\"../datasets/sns_datasets/titanic.csv\", source_column='sex')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HTML Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredHTMLLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = UnstructuredHTMLLoader(file_path=\"../datasets/harry_potter_html/001.htm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loader.load()\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import BSHTMLLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = BSHTMLLoader(file_path=\"../datasets/harry_potter_html/001.htm\")\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JSON Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "json_filepath = \"../datasets/population_data.json\"\n",
        "\n",
        "with open(json_filepath) as f:\n",
        "    loaded_json = json.loads(f.read())\n",
        "    \n",
        "len(loaded_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import JSONLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = JSONLoader(file_path=json_filepath, jq_schema=\"Value\")\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Markdown Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredMarkdownLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "md_filepath = \"../datasets/harry_potter_md/001.md\"\n",
        "\n",
        "os.path.isfile(md_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = UnstructuredMarkdownLoader(file_path=md_filepath)\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_filepath = \"../datasets/harry_potter_pdf/hpmor-trade-classic.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(file_path=pdf_filepath)\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[1].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integrations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WikipediaLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = WikipediaLoader(query='India', load_max_docs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ArXiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import ArxivLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = ArxivLoader(query='2201.03916', load_max_docs=1)\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the chat model\n",
        "\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.globals import set_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Ensure OPENAI_API_KEY is set in your .env file\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY in your .env file\")\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "system_template = \"You are a Peer Reviewer\"\n",
        "human_template = \"Read the paper with the title: '{title}'\\n\\nAnd Content: {content} and critically list down all the issues in the paper\"\n",
        "\n",
        "systemp_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([systemp_message_prompt, human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(title=data[0].metadata['Title'], content=data[0].metadata['Summary'])\n",
        "\n",
        "response = chat(messages = prompt.to_messages())\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def peer_review(article_id):\n",
        "    chat = ChatOpenAI()\n",
        "    loader = ArxivLoader(query=article_id, load_max_docs=2)\n",
        "    data = loader.load()\n",
        "    first_record = data[0]\n",
        "    page_content = first_record.page_content\n",
        "    title = first_record.metadata['Title']\n",
        "    summary = first_record.metadata['Summary']\n",
        "    \n",
        "    summary_list = []\n",
        "    for record in data:\n",
        "        summary_list.append(record.metadata['Summary'])\n",
        "    full_summary = \"\\n\\n\".join(summary_list)\n",
        "    \n",
        "    system_template = \"You are a Peer Reviewer\"\n",
        "    human_template = \"Read the paper with the title: '{title}'\\n\\nAnd Content: {content} and critically list down all the issues in the paper\"\n",
        "\n",
        "    systemp_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([systemp_message_prompt, human_message_prompt])\n",
        "    prompt = chat_prompt.format_prompt(title=title, content=page_content)\n",
        "\n",
        "    response = chat(messages = prompt.to_messages())\n",
        "\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(peer_review('1706.03762'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(peer_review('2201.03514'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a bot that can answer questions based on wikipedia articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filepath = \"../datasets/Harry Potter 1 - Sorcerer's Stone.txt\"\n",
        "\n",
        "with open(filepath, 'r') as f:\n",
        "    hp_book = f.read()\n",
        "    \n",
        "print(hp_book)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(hp_book)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(hp_book.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(hp_book.split(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(hp_book.split(\"\\n\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "line_len_list = []\n",
        "\n",
        "for line in hp_book.split('\\n\\n'):\n",
        "    curr_line_len = len(line)\n",
        "    line_len_list.append(curr_line_len)\n",
        "\n",
        "Counter(line_len_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character level splitting\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def len_func(text):\n",
        "    return len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(100 + 100 + 900) + 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1200,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len_func,\n",
        "    is_separator_regex=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "para_list = text_splitter.create_documents(texts=[hp_book])\n",
        "\n",
        "para_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "manual_character_split_chunks = []\n",
        "\n",
        "for para in hp_book.split(\"\\n\\n\"):\n",
        "    manual_character_split_chunks.append(para)\n",
        "    \n",
        "len(manual_character_split_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "langchain_character_split_chunks = []\n",
        "\n",
        "for para in para_list:\n",
        "    langchain_character_split_chunks.append(para)\n",
        "    \n",
        "len(langchain_character_split_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_chunk = para_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_chunk.metadata = {\"source\": filepath}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_chunk.metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res_para_list = []\n",
        "\n",
        "cnt = 0\n",
        "for para in para_list:\n",
        "    para.metadata = {\"source\": filepath, \"chunk_number\": cnt}\n",
        "    cnt += 1\n",
        "    res_para_list.append(para)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res_para_list[100].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extra_line = \" \".join(['word']*500)\n",
        "\n",
        "len(text_splitter.create_documents(texts = [extra_line + hp_book])[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter.create_documents(texts = [extra_line + hp_book])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recursive Character Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", ' '],\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap = 100,\n",
        "    length_function = len_func,\n",
        "    is_separator_regex=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n\".join([\"\\n\".join([\" \".join(['word']*100)]*20)]*10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_list = text_splitter.create_documents(texts = [extra_line + hp_book])\n",
        "\n",
        "chunk_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split by tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_sent = \"This is a sample sentence for you to tell me how the tokens are split in this sentence\"\n",
        "\n",
        "sample_sent.split(\" \")\n",
        "\n",
        "[\"Thi\", \"s\", \"is\", \"sample\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size = 1200,\n",
        "    chunk_overlap=100,\n",
        "    model_name = \"text-embedding-3-small\",\n",
        "    encoding_name= \"text-embedding-3-small\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_list = text_splitter.create_documents([hp_book])\n",
        "\n",
        "doc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[len(doc.page_content) for doc in doc_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_list = text_splitter.split_text(hp_book)\n",
        "\n",
        "doc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "res_doc_list = []\n",
        "\n",
        "for doc_txt in doc_list:\n",
        "    curr_doc = Document(page_content=doc_txt, metadata={\"source\": filepath})\n",
        "    res_doc_list.append(curr_doc)\n",
        "    \n",
        "res_doc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python_code = \"\"\"def peer_review(article_id):\n",
        "    chat = ChatOpenAI()\n",
        "    loader = ArxivLoader(query=article_id, load_max_docs=2)\n",
        "    data = loader.load()\n",
        "    first_record = data[0]\n",
        "    page_content = first_record.page_content\n",
        "    title = first_record.metadata['Title']\n",
        "    summary = first_record.metadata['Summary']\n",
        "    \n",
        "    summary_list = []\n",
        "    for record in data:\n",
        "        summary_list.append(record.metadata['Summary'])\n",
        "    full_summary = \"\\n\\n\".join(summary_list)\n",
        "    \n",
        "    system_template = \"You are a Peer Reviewer\"\n",
        "    human_template = \"Read the paper with the title: '{title}'\\n\\nAnd Content: {content} and critically list down all the issues in the paper\"\n",
        "\n",
        "    systemp_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([systemp_message_prompt, human_message_prompt])\n",
        "    prompt = chat_prompt.format_prompt(title=title, content=page_content)\n",
        "\n",
        "    response = chat(messages = prompt.to_messages())\n",
        "\n",
        "    return response.content\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# code splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON,\n",
        "    chunk_size=50,\n",
        "    chunk_overlap=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter.create_documents(texts = [python_code])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's start with OpenAI models\n",
        "\n",
        "import os\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables (should already be loaded from previous cells)\n",
        "# Ensure OPENAI_API_KEY is set in your .env file\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    if 'OPENAI_API_KEY' not in os.environ:\n",
        "        raise ValueError(\"Please set OPENAI_API_KEY in your .env file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_function = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"The scar had not pained Harry for nineteen years. All was well\"\n",
        "\n",
        "embedded_text = embedding_function.embed_query(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.array(embedded_text).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "doc_lines = [\n",
        "    Document(page_content=\"It is our choices, Harry, that show what we truly are, far more than our abilities\", metadata = {\"source\": \"Harry Potter\"}),\n",
        "    Document(page_content=text, metadata = {\"source\": \"Harry Potter\"}),\n",
        "]\n",
        "\n",
        "doc_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the page_content\n",
        "\n",
        "line_list = [doc.page_content for doc in doc_lines]\n",
        "\n",
        "line_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedded_docs = [embedding_function.embed_query(line) for line in line_list]\n",
        "\n",
        "np.array(embedded_docs).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedded_docs = embedding_function.embed_documents(line_list)\n",
        "\n",
        "np.array(embedded_docs).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MTEB leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "embedding_function = HuggingFaceBgeEmbeddings(\n",
        "    model_name = model_name,\n",
        "    model_kwargs = model_kwargs,\n",
        "    encode_kwargs = encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bge_embed_record = embedding_function.embed_query(\"This is some random text\")\n",
        "bge_embed_records = embedding_function.embed_documents([\"This is some random text\"])\n",
        "\n",
        "print(np.array(bge_embed_record).shape)\n",
        "print(np.array(bge_embed_records).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import FakeEmbeddings\n",
        "\n",
        "embedding_function = FakeEmbeddings(size=300)\n",
        "\n",
        "fake_embed_record = embedding_function.embed_query(\"This is some random text\")\n",
        "fake_embed_records = embedding_function.embed_documents([\"This is some random text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.array(fake_embed_record).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.array(fake_embed_records).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vectorstores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"chromadb==0.4.24\" \"faiss-cpu==1.8.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip show chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip show faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings, FakeEmbeddings\n",
        "from langchain_community.embeddings import FakeEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = WikipediaLoader(query='Elon Musk', load_max_docs=5)\n",
        "documents = loader.load()\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents=documents)\n",
        "print(len(docs))\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"BAAI/bge-large-en-v1.5\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "embedding_function = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "######### \n",
        "\n",
        "# embedding_function = FakeEmbeddings(size=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Who is Elon Musk's Father?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process\n",
        "# Query -> Query Embeddings\n",
        "# Chunks -> Chunk Embeddings -> Vectorstore\n",
        "# Query Embeddings and Chunk Embeddings will be matched to get the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FAISS (in memory database)\n",
        "\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = FAISS.from_documents(docs, embedding_function)\n",
        "\n",
        "# 'document in text' - embeddings\n",
        "# Query -> query embeddings -> match with the embeddings in the vector store -> return the text connected to those embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Querying\n",
        "\n",
        "matched_docs = db.similarity_search(query=query, k=5)\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[\"errol musk\" in doc.page_content.lower() for doc in matched_docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"../output/elon_musk_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the existing database\n",
        "\n",
        "loaded_db = Chroma(persist_directory=\"../output/elon_musk_db\", embedding_function=embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query\n",
        "\n",
        "print(query)\n",
        "\n",
        "matched_docs = db.similarity_search(query=query, k=3)\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adding the family information\n",
        "\n",
        "family_data_loader = WikipediaLoader(query=\"Musk Family\", load_max_docs=1)\n",
        "family_documents = family_data_loader.load()\n",
        "family_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "family_docs = text_splitter.split_documents(documents=family_documents)\n",
        "print(len(family_docs))\n",
        "family_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adding new information\n",
        "\n",
        "db = Chroma.from_documents(family_docs, embedding_function, persist_directory=\"../output/elon_musk_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs = db.similarity_search(query=query, k=4)\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deleting the information\n",
        "# Updating the information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()\n",
        "\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs = retriever.get_relevant_documents(query=query)\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How these retrievers should retreiver, how many items to retriever\n",
        "# MMR - Maximum marginal relevance\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 5})\n",
        "\n",
        "matched_docs = retriever.get_relevant_documents(query=query)\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs = {\"score_threshold\": 0.5})\n",
        "\n",
        "matched_docs = retriever.get_relevant_documents(query=query)\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db._collection.delete(ids=[\"1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs = {\"score_threshold\": 0.5})\n",
        "\n",
        "matched_docs = retriever.get_relevant_documents(query=query)\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Other Retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "chunk_size = 400\n",
        "chunk_overlap = 100\n",
        "\n",
        "# Loading the environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Loading the chat model\n",
        "chat = ChatOpenAI()\n",
        "\n",
        "# Loading data\n",
        "loader = WikipediaLoader(query='Steve Jobs', load_max_docs=5)\n",
        "documents = loader.load()\n",
        "\n",
        "# Text splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "docs = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "# Embedding function\n",
        "embedding_function = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Vector store\n",
        "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"../output/steve_jobs_db\")\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "query = \"When was Steve Jobs fired from Apple?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BM25 Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_retriever = BM25Retriever.from_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs = bm25_retriever.get_relevant_documents(\"Musk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# chunk_size = 400\n",
        "# chunk_overlap = 100\n",
        "\n",
        "# # Loading the environment variables\n",
        "# load_dotenv()\n",
        "\n",
        "# # Loading the chat model\n",
        "# chat = ChatOpenAI()\n",
        "\n",
        "# # Loading data\n",
        "# loader = WikipediaLoader(query='Steve Jobs', load_max_docs=5)\n",
        "# documents = loader.load()\n",
        "\n",
        "# # Text splitting\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "# docs = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "# # Embedding function\n",
        "# embedding_function = HuggingFaceBgeEmbeddings(\n",
        "#     model_name=\"BAAI/bge-large-en-v1.5\",\n",
        "#     model_kwargs={'device': 'cpu'},\n",
        "#     encode_kwargs={\"normalize_embeddings\": True}\n",
        "# )\n",
        "\n",
        "# # Vector store\n",
        "# db = Chroma.from_documents(docs, embedding_function, persist_directory=\"../output/steve_jobs_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"When was Steve Jobs fired from Apple?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MultiQuery Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mq_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mq_retriever.get_relevant_documents(query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(*[\"1. What year did Apple terminate Steve Jobs' employment?\", '2. At what point in time was Steve Jobs ousted from his position at Apple?',\n",
        "      '3. When did Steve Jobs experience his departure from Apple through termination?'], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retrieved_docs = mq_retriever.get_relevant_documents(query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "['1985' in doc.page_content for doc in retrieved_docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(retrieved_docs[1].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contextual Compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieval\n",
        "\n",
        "# Query -> get the responses\n",
        "\n",
        "# Query + responses -> LLM\n",
        "\n",
        "# Extract the relevant from the responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = Chroma(persist_directory=\"../output/steve_jobs.db\", embedding_function=embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_docs = retriever.get_relevant_documents(query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document compressor\n",
        "\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "compressor = LLMChainExtractor.from_llm(llm=chat)\n",
        "\n",
        "compressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(compressor.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compression Retriever\n",
        "\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
        "\n",
        "compression_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs = compression_retriever.get_relevant_documents(query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[len(doc.page_content) for doc in matched_docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parent Document Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split paragraph\n",
        "# split sentence\n",
        "# match sentences with query\n",
        "# get the paragraph with most matching sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parent_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=1000, chunk_overlap=100)\n",
        "child_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=200, chunk_overlap=50)\n",
        "\n",
        "store = InMemoryStore() # parent documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "par_doc_retriever = ParentDocumentRetriever(vectorstore=db, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "par_doc_retriever.add_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "par_doc_retriever.get_relevant_documents(query=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time-Weighted Vector Store Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matching_score = cosine_similarity + (1-decay_rate)^hours_passed\n",
        "\n",
        "0.9 + (0.1**5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "0.9 + 0.1**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
        "\n",
        "emb_size = 1024\n",
        "index = faiss.IndexFlatL2(emb_size)\n",
        "temp_db = FAISS(embedding_function, index, docstore=InMemoryDocstore({}), index_to_docstore_id={})\n",
        "\n",
        "tw_retriever = TimeWeightedVectorStoreRetriever(vectorstore=temp_db, decay_rate=1/1000000, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tw_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "five_hours_ago = datetime.now() - timedelta(hours=5)\n",
        "\n",
        "tw_retriever.add_documents(\n",
        "    [Document(page_content=\"What is John doing?\", metadata={\"last_accessed_at\": five_hours_ago})]\n",
        ")\n",
        "\n",
        "tw_retriever.add_documents([Document(page_content=\"What is Jack doing?\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tw_retriever.get_relevant_documents(\"What are you doing?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothetical Document Retreiver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VS -> documents (answers)\n",
        "# query -> query\n",
        "\n",
        "# matching -> query & answers\n",
        "# query + LLM -> hypothetical answer\n",
        "# matching -> hypothetical with actual answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question -> Embeddings\n",
        "# documents -> Embeddings\n",
        "# Match\n",
        "\n",
        "# Question -> Answer with LLM -> Embeddings\n",
        "# documents -> Embeddings\n",
        "# Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question -> Quesstion embeddings\n",
        "\n",
        "# docs -> document emb [1, 2, 3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "def get_hypo_doc(query):\n",
        "    template = \"\"\"Imagine you are an expert writing a detailed explanation on the topic: '{query}'\n",
        "    Your response should be comprehensive and include key points that would be found in a top search result.\"\"\"\n",
        "    \n",
        "    systemp_message_prompt = SystemMessagePromptTemplate.from_template(template=template)\n",
        "    \n",
        "    chat_prompt = ChatPromptTemplate.from_messages([systemp_message_prompt])\n",
        "    \n",
        "    messages = chat_prompt.format_prompt(query=query).to_messages()\n",
        "    \n",
        "    response = chat(messages=messages)\n",
        "    \n",
        "    hypo_doc = response.content\n",
        "    \n",
        "    return hypo_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_hypo_doc(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "matched_docs = base_retriever.get_relevant_documents(query=get_hypo_doc(query))\n",
        "\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import HypotheticalDocumentEmbedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyde_embedding_function = HypotheticalDocumentEmbedder.from_llm(llm=chat, base_embeddings=embedding_function, prompt_key='web_search')\n",
        "\n",
        "# web_search, sci_fact, arguana, trec_covid, fiqa, dbpedia, trec_news, mr_tydi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_db = Chroma.from_documents(docs, hyde_embedding_function, persist_directory=\"../output/steve_jobs_hyde\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs_new = doc_db.similarity_search(query)\n",
        "\n",
        "matched_docs_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs[0].page_content == matched_docs_new[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retriver1 (0.6) -> docA, docC, docH\n",
        "Retriver2 (0.4) -> docG, docY, docA\n",
        "\n",
        "score of docA: 1/1 + 1/3 -> 4/3\n",
        "score of docA: (1/1)*0.6 + (1/3)*0.4 -> x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reciprocal Rank Fusion\n",
        "\n",
        "from langchain.retrievers import EnsembleRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "par_doc_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, par_doc_retriever], weights=[0.5, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hybrid_matched_docs = ensemble_retriever.get_relevant_documents(query=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hybrid_matched_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding Redundant Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_transformers import EmbeddingsRedundantFilter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "redundant_filter.transform_documents(hybrid_matched_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embeddings Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "\n",
        "embeddings_filter = EmbeddingsFilter(embeddings = embedding_function)\n",
        "\n",
        "embeddings_filter.compress_documents(docs, query=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reordering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Long Context Reorder\n",
        "\n",
        "Important docs will be moved to beginning and the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_transformers import LongContextReorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reorder = LongContextReorder()\n",
        "\n",
        "reordered_docs = reorder.transform_documents(hybrid_matched_docs)\n",
        "\n",
        "reordered_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat Model\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loader\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(file_path=\"../datasets/udhr_booklet_en_web.pdf\")\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text Splitting\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "chunk_size = 500\n",
        "chunk_overlap = 100\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "docs = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding Function\n",
        "\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "embedding_function = HuggingFaceBgeEmbeddings(\n",
        "    model_name = model_name,\n",
        "    model_kwargs = model_kwargs,\n",
        "    encode_kwargs = encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector store\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"../output/human_rights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How does the declaration address the discrimination?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "logging.basicConfig()\n",
        "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contextual Compression + Multi-query retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compressor\n",
        "\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "base_compressor = LLMChainExtractor.from_llm(llm=chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-Query Retriever\n",
        "\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "base_retriever = db.as_retriever()\n",
        "mq_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "mq_compression_retriever = ContextualCompressionRetriever(base_compressor=base_compressor, base_retriever=mq_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_docs = mq_compression_retriever.get_relevant_documents(query=query)\n",
        "matched_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_content = \"\"\n",
        "\n",
        "for doc in matched_docs:\n",
        "    page_content = doc.page_content\n",
        "    matched_content += page_content\n",
        "    matched_content += \"\\n\\n\"\n",
        "    \n",
        "print(matched_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Augmentation\n",
        "\n",
        "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the following the question only by using the content given below in the triple backticks, do not use any other information to answer the question. If you can't answer the given question with the given context, you can return 'NO_OUTPUT' as a string.\n",
        "\n",
        "Context: ```{context}```\n",
        "-------------------------------\n",
        "Question: {query}\n",
        "-------------------------------\n",
        "Answer: \"\"\"\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(template=template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(query=query, context=matched_content)\n",
        "messages = prompt.to_messages()\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation\n",
        "response = chat(messages=messages).content\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compressor -> HyDE + redundant filter + reordering\n",
        "# Retriever -> Ensemble Retriever (Multi-query retriever, Tfidf, Parent Document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compressor\n",
        "\n",
        "from langchain.chains import HypotheticalDocumentEmbedder\n",
        "from langchain.document_transformers import EmbeddingsRedundantFilter, LongContextReorder\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "\n",
        "hyde_embedding_function = HypotheticalDocumentEmbedder.from_llm(llm=chat, base_embeddings=embedding_function, prompt_key='web_search')\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=hyde_embedding_function)\n",
        "lcr = LongContextReorder()\n",
        "\n",
        "compression_pipeline = DocumentCompressorPipeline(transformers = [redundant_filter, lcr])\n",
        "\n",
        "compression_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrievers\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers import TFIDFRetriever, MultiQueryRetriever, ParentDocumentRetriever, EnsembleRetriever, ContextualCompressionRetriever\n",
        "\n",
        "## TFIDF\n",
        "tfidf_retriever = TFIDFRetriever.from_documents(docs)\n",
        "\n",
        "## Multi-Query\n",
        "mq_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=chat)\n",
        "\n",
        "## Parent document Retriever\n",
        "parent_splitter= RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\n",
        "child_splitter= RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "store = InMemoryStore()\n",
        "\n",
        "### Creating an instance of parent-document retriever\n",
        "par_doc_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=db,\n",
        "    docstore=store, \n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter\n",
        ")\n",
        "\n",
        "par_doc_retriever.add_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emsemble Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_pipeline = EnsembleRetriever(retrievers = [tfidf_retriever, mq_retriever, par_doc_retriever], weights=[0.4, 0.3, 0.3])\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compression_pipeline, base_retriever=retriever_pipeline)\n",
        "\n",
        "matching_docs = compression_retriever.get_relevant_documents(query=query)\n",
        "matching_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieval QA Chain\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=chat,\n",
        "    chain_type='stuff',\n",
        "    retriever=compression_retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = qa_chain(query)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TASK\n",
        "\n",
        "# Private GPT"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
